#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Script ƒë·ªÉ ingest d·ªØ li·ªáu t√†i ch√≠nh v√†o Qdrant Vector Database

Usage:
    python ingest_financial_data.py VIC
    python ingest_financial_data.py HPG FPT VIC
"""

import sys
import requests
from qdrant_client import QdrantClient, models
from config import Config
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# ---------- EMBEDDING ----------
def get_embedding(text: str):
    """Get embedding for a single text"""
    headers = {
        "Authorization": f"Bearer {Config.OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": Config.EMBEDDING_MODEL,
        "input": text
    }
    resp = requests.post(f"{Config.OPENAI_BASE}/embeddings", headers=headers, json=payload)
    resp.raise_for_status()
    return resp.json()["data"][0]["embedding"]

def get_embeddings_batch(texts: list):
    """Get embeddings for multiple texts in one API call (MUCH FASTER!)"""
    if not texts:
        return []
    
    headers = {
        "Authorization": f"Bearer {Config.OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": Config.EMBEDDING_MODEL,
        "input": texts  # Send all texts at once
    }
    resp = requests.post(f"{Config.OPENAI_BASE}/embeddings", headers=headers, json=payload)
    resp.raise_for_status()
    
    # Response includes embeddings in order
    data = resp.json()["data"]
    return [item["embedding"] for item in sorted(data, key=lambda x: x["index"])]


# ---------- L·∫§Y D·ªÆ LI·ªÜU IQX ----------
def fetch_company_data(ticker: str):
    base = f"https://proxy.iqx.vn/proxy/trading/api/iq-insight-service/v1/company/{ticker}"
    sections = [
        "financial-statement?section=CASH_FLOW",
        "financial-statement?section=INCOME_STATEMENT",
        "financial-statement?section=BALANCE_SHEET",
        "statistics-financial",
    ]

    data_blocks = []
    field_map = {}  # L∆∞u mapping field name -> readable name
    
    for s in sections:
        url = f"{base}/{s}"
        print(f"üîπ Fetching {url}")
        r = requests.get(url)
        if r.status_code == 200:
            content = r.json()
            data_blocks.append({"section": s, "content": content})
            
            # N·∫øu l√† metrics, l∆∞u field mapping
            if s == "financial-statement/metrics" and "data" in content:
                for category, items in content["data"].items():
                    if isinstance(items, list):
                        for item in items:
                            if isinstance(item, dict):
                                name = item.get("name", "")
                                title = item.get("titleVi") or item.get("titleEn", "")
                                if name and title:
                                    field_map[name.lower()] = title
        else:
            print(f"‚ö†Ô∏è  L·ªói khi fetch {s}: {r.status_code}")
    
    return data_blocks, field_map


# ---------- CHUY·ªÇN JSON TH√ÄNH TEXT ----------
def flatten_json_to_text(section, data, field_map=None):
    texts = []
    if not isinstance(data, dict):
        return texts
    if "data" not in data:
        return texts

    raw_data = data["data"]
    
    # Case 1: data is a list (statistics-financial) - G·ªòP THEO K·ª≤ B√ÅO C√ÅO
    if isinstance(raw_data, list):
        for item in raw_data:
            if not isinstance(item, dict):
                continue
            year = item.get("year", "")
            quarter = item.get("quarter", "")
            period = f"Q{quarter}/{year}" if quarter and year else str(year) if year else ""
            
            # G·ªôp c√°c ch·ªâ s·ªë quan tr·ªçng v√†o 1 ƒëo·∫°n text
            key_metrics = []
            important_fields = [
                "marketCap", "pe", "pb", "ps", "roe", "roa", "eps", "bvps",
                "grossMargin", "ebitMargin", "afterTaxProfitMargin",
                "currentRatio", "quickRatio", "debtPerEquity", "debtToEquity",
                "revenue", "grossProfit", "netProfit", "totalAssets", "totalEquity"
            ]
            
            for key in important_fields:
                value = item.get(key)
                if value is not None and value != "":
                    key_metrics.append(f"{key}: {value}")
            
            if key_metrics and period:
                line = f"{section} ({period})\n" + "\n".join(key_metrics)
                texts.append(line)
    
    # Case 2: data is a dict with categories (financial-statement)
    elif isinstance(raw_data, dict):
        # Case 2A: financial-statement v·ªõi gi√° tr·ªã th·ª±c t·∫ø (c√≥ key 'years')
        if 'years' in raw_data and isinstance(raw_data['years'], list):
            # S·ª≠ d·ª•ng field_map ƒë∆∞·ª£c truy·ªÅn v√†o (t·ª´ financial-statement/metrics)
            
            # X·ª≠ l√Ω t·ª´ng nƒÉm
            for year_data in raw_data['years'][-3:]:  # L·∫•y 3 nƒÉm g·∫ßn nh·∫•t
                year = year_data.get('yearReport', '')
                if not year:
                    continue
                
                # L·∫•y c√°c ch·ªâ ti√™u quan tr·ªçng
                year_metrics = []
                important_prefixes = ['cfa', 'isa', 'bsa']  # CASH_FLOW, INCOME_STATEMENT, BALANCE_SHEET
                
                for key, value in year_data.items():
                    if value is None or value == "" or value == 0:
                        continue
                    # Ch·ªâ l·∫•y c√°c field b·∫Øt ƒë·∫ßu b·∫±ng cfa, isa, bsa
                    if any(key.lower().startswith(prefix) for prefix in important_prefixes):
                        if field_map:
                            field_name = field_map.get(key.lower(), key.upper())
                        else:
                            field_name = key.upper()
                        year_metrics.append(f"{key.upper()}: {field_name} = {value:,.0f}")
                
                if year_metrics and year:
                    # Gi·ªõi h·∫°n 30 ch·ªâ ti√™u quan tr·ªçng nh·∫•t
                    line = f"{section} - NƒÉm {year}\n" + "\n".join(year_metrics[:30])
                    texts.append(line)
        
        # Case 2B: financial-statement/metrics (ch·ªâ metadata)
        else:
            for category, items in raw_data.items():
                if not isinstance(items, list):
                    continue
                
                # G·ªôp c√°c ch·ªâ ti√™u trong c√πng category
                category_items = []
                for item in items[:50]:  # Gi·ªõi h·∫°n 50 item ƒë·∫ßu ti√™n
                    if not isinstance(item, dict):
                        continue
                    title = item.get("titleVi") or item.get("titleEn", "")
                    name = item.get("name", "")
                    if title and name:
                        category_items.append(f"{name}: {title}")
                
                if category_items:
                    line = f"{section} - {category}\n" + "\n".join(category_items)
                    texts.append(line)
    
    return texts


# ---------- N·∫†P D·ªÆ LI·ªÜU V√ÄO QDRANT ----------
def ingest_to_qdrant(ticker="VIC", recreate_collection=True):
    """
    Ingest financial data to Qdrant
    
    Args:
        ticker: Stock ticker symbol
        recreate_collection: If True, delete and recreate collection. 
                           If False, add to existing collection.
    """
    # 1Ô∏è‚É£ K·∫øt n·ªëi
    # H·ªó tr·ª£ c·∫£ local (host+port) v√† remote (URL)
    if Config.QDRANT_HOST.startswith(('http://', 'https://')):
        # Remote Qdrant - parse URL to extract host
        from urllib.parse import urlparse
        parsed = urlparse(Config.QDRANT_HOST)
        host = parsed.hostname or parsed.netloc
        port = parsed.port or (443 if parsed.scheme == 'https' else 80)
        use_https = (parsed.scheme == 'https')
        
        qdrant = QdrantClient(
            host=host,
            port=port,
            https=use_https,
            api_key=Config.QDRANT_API_KEY,
            timeout=60,
            prefer_grpc=False
        )
        print(f"üîó K·∫øt n·ªëi Qdrant: {host}:{port} (https={use_https})")
    else:
        # Local Qdrant
        qdrant = QdrantClient(host=Config.QDRANT_HOST, port=Config.QDRANT_PORT)
        print(f"üîó K·∫øt n·ªëi Qdrant local: {Config.QDRANT_HOST}:{Config.QDRANT_PORT}")

    # 2Ô∏è‚É£ T·∫°o/ki·ªÉm tra collection
    collection_name = Config.QDRANT_COLLECTION
    
    if recreate_collection:
        if qdrant.collection_exists(collection_name):
            print(f"üóëÔ∏è  X√≥a collection c≈©: {collection_name}")
            qdrant.delete_collection(collection_name)
        
        print(f"üÜï T·∫°o collection m·ªõi: {collection_name}")
        qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE)
        )
    else:
        if not qdrant.collection_exists(collection_name):
            print(f"üÜï Collection ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi: {collection_name}")
            qdrant.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(size=3072, distance=models.Distance.COSINE)
            )
        else:
            print(f"‚ûï Th√™m d·ªØ li·ªáu v√†o collection hi·ªán t·∫°i: {collection_name}")

    # 3Ô∏è‚É£ L·∫•y d·ªØ li·ªáu IQX
    print(f"\nüìà ƒêang l·∫•y d·ªØ li·ªáu cho m√£ {ticker}...")
    data_blocks, field_map = fetch_company_data(ticker)
    print(f"üìã ƒê√£ load {len(field_map)} field mappings")

    # 4Ô∏è‚É£ T·∫°o text list
    all_texts = []
    for block in data_blocks:
        texts = flatten_json_to_text(block["section"], block["content"], field_map)
        for t in texts:
            all_texts.append({"text": t, "section": block["section"]})
    
    if not all_texts:
        print(f"‚ö†Ô∏è  Kh√¥ng c√≥ d·ªØ li·ªáu cho {ticker}, b·ªè qua")
        return
    
    print(f"üì¶ T√¨m th·∫•y {len(all_texts)} ƒëi·ªÉm d·ªØ li·ªáu")
    
    # Get offset ID for multi-ticker ingestion
    if not recreate_collection:
        try:
            scroll_result = qdrant.scroll(collection_name=collection_name, limit=1, with_payload=False, with_vectors=False)
            if scroll_result[0]:
                offset_id = max([p.id for p in scroll_result[0]]) + 1
            else:
                offset_id = 0
        except:
            offset_id = 0
    else:
        offset_id = 0
    
    # 5Ô∏è‚É£ Get embeddings in batches (MUCH FASTER!)
    print(f"üöÄ ƒêang t·∫°o embeddings (batch mode - nhanh h∆°n 50-100x)...")
    BATCH_SIZE = 50  # OpenAI supports up to 2048, but 50 is safer
    all_embeddings = []
    
    for i in range(0, len(all_texts), BATCH_SIZE):
        batch = all_texts[i:i+BATCH_SIZE]
        batch_texts = [item["text"] for item in batch]
        
        try:
            embeddings = get_embeddings_batch(batch_texts)
            all_embeddings.extend(embeddings)
            print(f"üìä ƒê√£ x·ª≠ l√Ω {min(i+BATCH_SIZE, len(all_texts))}/{len(all_texts)} embeddings...")
        except Exception as e:
            print(f"‚ùå L·ªói batch {i}-{i+BATCH_SIZE}: {e}")
            # Fallback to single embeddings for this batch
            for text in batch_texts:
                try:
                    emb = get_embedding(text)
                    all_embeddings.append(emb)
                except Exception as e2:
                    print(f"‚ùå L·ªói single embedding: {e2}")
                    all_embeddings.append([0.0] * 3072)  # Zero vector as fallback
    
    # 6Ô∏è‚É£ Create points
    print(f"üìù T·∫°o {len(all_embeddings)} points...")
    points = []
    for idx, (item, emb) in enumerate(zip(all_texts, all_embeddings)):
        point = models.PointStruct(
            id=offset_id + idx,
            vector=emb,
            payload={"ticker": ticker, "text": item["text"], "section": item["section"]}
        )
        points.append(point)

    # 7Ô∏è‚É£ Ghi v√†o Qdrant
    print(f"üíæ ƒêang ghi {len(points)} ƒëi·ªÉm v√†o Qdrant...")
    qdrant.upsert(collection_name=collection_name, points=points)
    print(f"‚úÖ ƒê√£ n·∫°p {len(points)} ƒëo·∫°n d·ªØ li·ªáu cho {ticker}.\n")


if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ FINANCIAL DATA INGESTION SCRIPT")
    print("=" * 60)
    
    if len(sys.argv) < 2:
        print("\n‚ùå S·ª≠ d·ª•ng: python ingest_financial_data.py <TICKER1> [TICKER2] ...")
        print("V√≠ d·ª•: python ingest_financial_data.py VIC")
        print("V√≠ d·ª•: python ingest_financial_data.py VIC HPG FPT\n")
        sys.exit(1)
    
    tickers = sys.argv[1:]
    print(f"\nüìã Danh s√°ch m√£ c·ªï phi·∫øu: {', '.join(tickers)}")
    print(f"üîß Qdrant: {Config.QDRANT_HOST}:{Config.QDRANT_PORT}")
    print(f"üì¶ Collection: {Config.QDRANT_COLLECTION}\n")
    
    for i, ticker in enumerate(tickers):
        ticker = ticker.upper()
        print(f"\n{'='*60}")
        print(f"[{i+1}/{len(tickers)}] Processing {ticker}")
        print(f"{'='*60}")
        
        # First ticker: recreate collection, rest: add to collection
        recreate = (i == 0)
        ingest_to_qdrant(ticker, recreate_collection=recreate)
    
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN T·∫§T T·∫§T C·∫¢!")
    print("="*60)

